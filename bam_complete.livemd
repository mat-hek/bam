<!-- livebook:{"file_entries":[{"name":"bambam.excalidraw.png","type":"attachment"},{"name":"bottom_left.png","type":"attachment"},{"name":"top_right.png","type":"attachment"}]} -->

# Project bam bam - complete

```elixir
Logger.configure(level: :info)

Mix.install([
  :evision,
  :membrane_sdk,
  {:membrane_webrtc_plugin, "~> 0.19.0"},
  {:ex_sdp, "~> 0.15.0", override: true},
  {:utils, path: "#{__DIR__}/utils"},
  :kino
])
```

## Project bam bam

* Record hand movements with camera
* Stream it over WebRTC
* Detect hand position with AI
* Emit _bam_ sound whenever the hand moves down
* Stream the sound back

![](files/bambam.excalidraw.png)

## Stream from the client

* Use `getUserMedia` to get the video from the browser
* Use JS WebRTC API to send it to the server
* Use JS WebRTC API to receive audio from the server
* Put it into the HTML `<audio/>` element

<!-- livebook:{"branch_parent_index":0} -->

## Receive media on the server

* Receive video over WebRTC
* Parse the video
* Decode the video
* Convert the video from YUV to RGB https://deeprender.ai/blog/yuv-colour-and-compression

```elixir
import Membrane.ChildrenSpec

p = Membrane.RCPipeline.start_link!()

Membrane.RCPipeline.exec_actions(p,
  spec:
    child(%Membrane.WebRTC.Source{signaling: {:websocket, port: 8829}, video_codec: :h264})
    |> via_out(:output, options: [kind: :video])
    |> child(Membrane.H264.Parser)
    |> child(Membrane.H264.FFmpeg.Decoder)
    |> child(%Membrane.FFmpeg.SWScale.PixelFormatConverter{format: :RGB})
    |> child(%Membrane.Debug.Sink{handle_stream_format: &IO.inspect/1})
)
```

```elixir
Membrane.Pipeline.terminate(p)
```

<!-- livebook:{"branch_parent_index":0} -->

## Detect hand movement

* Models prepared by Florian Bruggisser, source: https://github.com/cansik/yolo-hand-detection
* Using [Evision](https://github.com/cocoa-xu/evision) to do the inference

```elixir
hand_top_right = Image.open!("/Users/matheksm/Downloads/top_right.png")
hand_bottom_left = Image.open!("/Users/matheksm/Downloads/bottom_left.png")

Kino.Layout.grid([hand_top_right, hand_bottom_left], columns: 2)
```

```elixir
defmodule BamModel do
  @type t :: %{network: Evision.DNN.Net.t(), output_layers: [String.t()]}

  @type inference :: %{x: float(), y: float(), score: float()}

  @spec load() :: t()
  def load() do
    path = "#{__DIR__}/../models"

    network =
      Evision.DNN.readNetFromDarknet("#{path}/cross-hands-tiny.cfg",
        darknetModel: "#{path}/cross-hands-tiny.weights"
      )

    output_layers = Evision.DNN.Net.getUnconnectedOutLayersNames(network)

    %{network: network, output_layers: output_layers}
  end

  @spec infer(binary(), %{width: pos_integer, height: pos_integer}, t()) :: inference
  def infer(image, format, model) do
    img = Evision.Mat.from_binary(image, {:u, 8}, format.height, format.width, 3)

    blob =
      Evision.DNN.blobFromImage(img, scalefactor: 1.0 / 255.0, size: {416, 416})

    Evision.DNN.Net.setInput(model.network, blob)
    outputs = Evision.DNN.Net.forwardAndRetrieve(model.network, model.output_layers)

    outputs
    |> List.flatten()
    |> Enum.map(&Evision.Mat.to_nx/1)
    |> Enum.flat_map(&Nx.to_list/1)
    |> Enum.map(fn [x, y, _w, h | scores] ->
      %{x: x, y: y - h / 2, score: Enum.max(scores)}
    end)
    |> Enum.max_by(& &1.score)
  end
end

model = BamModel.load()

image = hand_bottom_left
{:ok, image_binary} = image |> Image.flatten!() |> Vix.Vips.Image.write_to_binary()

BamModel.infer(
  image_binary,
  %{width: Image.width(image), height: Image.height(image)},
  model
)
```

```elixir
defmodule BamDetector do
  @type state :: number()

  @spec init_state() :: state
  def init_state() do
    1
  end

  @spec detect(BamModel.inference(), state) :: {boolean, state}
  def detect(inference, detector_state) do
    cond do
      inference.score < 0.5 -> {false, detector_state}
      inference.y < detector_state -> {false, inference.y}
      inference.y - detector_state > 0.2 -> {true, init_state()}
      true -> {false, detector_state}
    end
  end
end
```

```elixir
defmodule BamSound do
  @spec format() :: Membrane.RawAudio.t()
  def format() do
    %Membrane.RawAudio{sample_rate: 48_000, channels: 2, sample_format: :s16le}
  end

  @spec bam() :: binary()
  def bam() do
    File.read!("#{__DIR__}/clap.pcm")
  end
end
```

```elixir
defmodule BamBam do
  use Membrane.Filter

  def_input_pad(:input, accepted_format: Membrane.RawVideo)
  def_output_pad(:output, accepted_format: Membrane.RawAudio)

  @impl true
  def handle_init(_ctx, _opts) do
    state = %{
      input_format: nil,
      model: BamModel.load(),
      detector_state: BamDetector.init_state(),
      bam: BamSound.bam(),
      processing: false
    }

    {[], state}
  end

  @impl true
  def handle_stream_format(:input, format, _ctx, state) do
    {[stream_format: {:output, BamSound.format()}], %{state | input_format: format}}
  end

  @impl true
  def handle_buffer(:input, buffer, _ctx, state) do
    if state.processing == false do
      element = self()

      Task.start_link(fn ->
        inference = BamModel.infer(buffer.payload, state.input_format, state.model)
        send(element, inference)
      end)
    end

    {[], %{state | processing: true}}
  end

  @impl true
  def handle_info(inference, _ctx, state) do
    {is_bam, detector_state} = BamDetector.detect(inference, state.detector_state)
    state = %{state | processing: false, detector_state: detector_state}

    if is_bam do
      buffer = %Membrane.Buffer{payload: state.bam}
      {[buffer: {:output, buffer}], state}
    else
      {[], state}
    end
  end
end
```

## Plug hand detection & stream back

* Plug the new element into the pipeline
* Fill audio gaps with silence
* Encode the audio
* Send the audio via WebRTC

```elixir
import Membrane.ChildrenSpec

p = Membrane.RCPipeline.start_link!()

Membrane.RCPipeline.exec_actions(p,
  spec:
    child(%Membrane.WebRTC.Source{signaling: {:websocket, port: 8829}, video_codec: :h264})
    |> via_out(:output, options: [kind: :video])
    |> child(Membrane.H264.Parser)
    |> child(Membrane.H264.FFmpeg.Decoder)
    |> child(%Membrane.FFmpeg.SWScale.PixelFormatConverter{format: :RGB})
    |> child(%Membrane.Debug.Filter{handle_stream_format: &IO.inspect/1})
    |> child(BamBam)
    |> child(Utils.SilenceFiller)
    |> child(Membrane.Opus.Encoder)
    |> via_in(:input, options: [kind: :audio])
    |> child(%Membrane.WebRTC.Sink{signaling: {:websocket, port: 8830}})
)
```

```elixir
Membrane.Pipeline.terminate(p)
```

<!-- livebook:{"branch_parent_index":0} -->

## WebRTC & low latency streaming

| &nbsp;                     | Sending a file    | Low-latency media streaming |
| -------------------------- | ----------------- | --------------------------- |
| Packet loss                | Always retransmit | Maybe drop it, FEC          |
| Congestion decection       | On packet loss    | Constant monitoring         |
| Recovering from congestion | Slow down         | Reduce quality              |
| Transport protocol         | TCP               | Custom stack on top of UDP  |
